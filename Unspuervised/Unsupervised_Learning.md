# 12/29

# 비지도 학습 Unsupervised learning

- 레이블이 없는 데이터 안에서 패턴과 구조를 발견하는 학습
    - 레이블 = 타겟
        - 최종적인 타겟 데이터가 분류용 데이터면 레이블
        - 회귀 문제(연속형, 숫자)면 타깃
    - 정답 값이 없는 데이터를 가지고 학습
    - 정답을 맞추기 위한 알고리즘은 아님
- 갖고 있는 데이터 안에 어떤 패턴을 찾는 것이 목적
- 분류: 정답이 있는 상태에서
- 군집화Clustering: 정답이 없음, 데이터 안에서 패턴을 판단
- 데이터 전처리 시 clustering을 돌려서 쓰레기값 데이터들을 거를 수 있음

## K-Means Clustering (K-평균 군집화)

- 업무적으로 많이 사용하지는 않음

### 군집화

- 목표: 서로 유사한 데이터들을 같은 그룹으로 묶어주는 것
- K: 몇 개의 그룹으로 묶을지
- 몇 개의 그룹으로 묶을지 + 유사도 정의는 학습시키는 사람이 지정해줘야 함
    - hyper parameter
    - K 값을 잘 정하는 게 중요
- 거리를 베이스로 함
    - 스케일링을 해야 함

### K-Means Clustering

- K개의 임의의 중심점을 배치
    - 각 중심점 사이의 거리가 최대한 멀어지도록(k-means++)
    - k를 정함 → 임의로 좌표에 찍힘
- 데이터마다 중심점들과의 거리를 측정
- 각 데이터들을 가장 가까운 중심점으로 할당
- 중심점을 각 군집의 중심으로 이동
- 더 이상 중심점의 이동이 없을 때까지 반복
- 단점
    - 시간이 많이 걸림
    - 일일이 거리 계산 - 데이터가 많을 수록 시간이 많이 걸림

### SSE(Sum of Squared Error)

- elbow method
- 적당한 k를 선택하는 방법
- 관측지와 중심들 사이의 거리
    - 의 합
    - 작을수록 grouping이 잘 된 것
- 군집들이 얼만큼 뭉쳐있는지를 나타내는 지표
- 함수 그래프로 그렸을 때 꺾이는 지점 = Elbow point로 최적의 k값

### Silhouette Score

- 개체가 다른 클러스터에 비해 자신의 클러스터와 얼마나 유사한지 측정
    - 자신 그룹에 속해있는 어떤 값과 다른 그룹에 있는 데이터 사이의 거리
- 실루엣 범위: -1 ~ 1
    - 값이 높으면 객체가 자체 클러스터와 잘 일치하는 것

## 차원 축소

### 차원

: 공간에서 데이터의 위치를 나타내기 위해 필요한 축(axis)의 수

축(axis)의 수 = 속성의 수

### 차원의 저주

- 차원이 증가 → 정보 밀도 감수
    - 필요한 데이터의 수가 지수 함수적으로 증가
- 정보의 감소로 과적합 문제 발생

### 해결 방법

- 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키움
    - 차원이 증가하면서 필요한 데이터의 수가 지수 함수적으로 증가 → 불가능한 경우 多
- 차원 축소
    - 특징 선택 = 변수 선택: 안 쓸 속성 지움
    - 특징 추출: 원본 데이터의 특징을 조합해서 새로운 특징을 만듦

### 투영Projection

: 차원 축소 알고리즘의 주요한 접근법의 하나

### 주성분 분석 Principal Component Analysis

- 주성분: 데이터의 분산을 가장 잘 유지하는 축
- 주성분으로 투영
- 가장 인기 있는 차원 축소 알고리즘
- 분산ㄴ이 최대로 되는 축 선택→축에 대해 투영
- 첫번째 축에 직교하고 남은 분산을 최대화 보존하는 두번재 축을 찾음

단점

- 데이터 분포에 선형성이 없다면 적용X
    - 데이터가 모여 있는 경우 축을 잡아도 데이터들끼리 겹침
- 데이터의 클래스 고려 X → 최대 분산 방향이 특징 구분을 좋게 한다고 보장X
    - 투영 기준 = 축
